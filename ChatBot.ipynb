{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un_YlDiQavIe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@misc{traditional-chinese-alpaca,\n",
        "  author = {Wei-Lin Chen and Cheng-Kuang Wu and Hsin-Hsi Chen},\n",
        "  title = {Traditional-Chinese Alpaca: Models and Datasets},\n",
        "  year = {2023},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\\\url{https://github.com/ntunlplab/traditional-chinese-alpaca}},\n",
        "}\n",
        "本程式使用了此研究提供的資料集並參考其部分程式碼。\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "metadata": {
        "id": "5vYtVqTWavwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo add-apt-repository ppa:ubuntu-toolchain-r/test\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install --only-upgrade libstdc++6\n",
        "!strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX\n",
        "! pip install transformers datasets torch trl bitsandbytes\n",
        "!Y| pip uninstall peft\n",
        "! pip install peft==0.9.0"
      ],
      "metadata": {
        "id": "rT0YnJ9iaxe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ntunlplab/traditional-chinese-alpaca/main/data/alpaca-tw_en_instruction.json\n",
        "!mkdir chatbot_model"
      ],
      "metadata": {
        "id": "Ce6h__O2ayl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize a new W&B run to track this job\n",
        "run = wandb.init(project=\"huggingface\", job_type=\"model_training\")\n",
        "\n",
        "# Create a new artifact, which is a sample dataset\n",
        "dataset = wandb.Artifact('alpaca-tw_en', type='dataset')\n",
        "# Add files to the artifact, in this case a simple text file\n",
        "dataset.add_file('alpaca-tw_en_instruction.json')\n",
        "# Log the artifact to save it as an output of this run\n",
        "run.log_artifact(dataset)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "T22ppX5Baz2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from peft import PeftModel\n",
        "\n",
        "from transformers import (\n",
        "  BertTokenizerFast,\n",
        "  AutoModel,\n",
        "  AutoTokenizer,\n",
        "  GPT2LMHeadModel,\n",
        "  TrainingArguments,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainerCallback\n",
        ")\n",
        "from peft import (\n",
        "    #prepare_model_for_int8_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        ")\n",
        "import wandb\n",
        "MICRO_BATCH_SIZE = 4\n",
        "BATCH_SIZE = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "# EPOCHS = 6\n",
        "LEARNING_RATE = 3e-4\n",
        "CUTOFF_LEN = 256\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "VAL_SET_SIZE = 0\n",
        "TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "]\n",
        "\n",
        "class WandbUploadCallback(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        arti_model = wandb.Artifact('gpt2', type='model')\n",
        "        arti_model.add_dir('chatbot_model')\n",
        "        wandb.log_artifact(arti_model)\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    # This function masks out the labels for the input,\n",
        "    # so that our loss is computed only on the response.\n",
        "    user_prompt = (\n",
        "        (\n",
        "            f\"\"\"下方是一個關於任務的指令，以及一個提供與任務相關之資訊的輸入。請撰寫一個能適當地完成該任務需求的回覆。\n",
        "            ### 指令:\n",
        "            {data_point[\"instruction\"]}\n",
        "            ### 輸入:\n",
        "            {data_point[\"input\"]}\n",
        "            ### 回覆:\n",
        "            \"\"\"\n",
        "        )\n",
        "        if data_point[\"input\"]\n",
        "        else (\n",
        "            f\"\"\"下方是一個關於任務的指令。請撰寫一個能適當地完成該任務需求的回覆。\n",
        "            ### 輸入:\n",
        "            {data_point[\"instruction\"]}\n",
        "            ### 回覆:\n",
        "            \"\"\"\n",
        "        )\n",
        "    )\n",
        "    len_user_prompt_tokens = (\n",
        "        len(\n",
        "            tokenizer(\n",
        "                user_prompt,\n",
        "                truncation=True,\n",
        "                max_length=CUTOFF_LEN + 1,\n",
        "                padding=\"max_length\",\n",
        "            )[\"input_ids\"]\n",
        "        )\n",
        "        - 1\n",
        "    )  # no eos token\n",
        "    full_tokens = tokenizer(\n",
        "        user_prompt + data_point[\"output\"],\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )[\"input_ids\"][:-1]\n",
        "    return {\n",
        "        \"input_ids\": full_tokens,\n",
        "        \"labels\": [-100] * len_user_prompt_tokens\n",
        "        + full_tokens[len_user_prompt_tokens:],\n",
        "        \"attention_mask\": [1] * (len(full_tokens)),\n",
        "    }\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    device_map = \"auto\"\n",
        "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
        "    ddp = world_size != 1\n",
        "    wandb.init(\n",
        "        project='huggingface',\n",
        "        name='batch_size: %d' % BATCH_SIZE,\n",
        "        resume = 'allow')\n",
        "    arti_dataset = wandb.Artifact('alpaca-tw_en_instruction', type='dataset')\n",
        "    arti_dataset.add_file('alpaca-tw_en_instruction.json')\n",
        "    wandb.log_artifact(arti_dataset)\n",
        "\n",
        "    arti_model = wandb.Artifact('gpt2', type='model')\n",
        "    arti_model.add_dir('chatbot_model')\n",
        "    wandb.log_artifact(arti_model)\n",
        "    model_name = 'gpt2'\n",
        "    train_set = ''\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    try:\n",
        "        best_model = wandb.restore('gpt2',run_path = '%my W&B ProjectName%')\n",
        "        model.load_weights(best_model.name)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    if train_set =='bst':\n",
        "        dataset = load_dataset(\"blended_skill_talk\", split = 'train')\n",
        "\n",
        "        def format_conversation(example):\n",
        "            formatted_text = \"\"\n",
        "            if example[\"previous_utterance\"]:\n",
        "                formatted_text += f\"A: {example['previous_utterance']}\\n\"\n",
        "\n",
        "            for i, msg in enumerate(example[\"free_messages\"]):\n",
        "                speaker = \"A\" if i % 2 == 0 else \"B\"\n",
        "                formatted_text += f\"{speaker}: {msg}\\n\"\n",
        "\n",
        "            for i, msg in enumerate(example[\"guided_messages\"]):\n",
        "                speaker = \"A\" if i % 2 == 0 else \"B\"\n",
        "                formatted_text += f\"{speaker}: {msg}\\n\"\n",
        "\n",
        "            return {\"text\": formatted_text}\n",
        "\n",
        "        train_dataset = dataset.map(format_conversation)\n",
        "    elif train_set == 'wiki':\n",
        "        train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\",split = 'train').select(range(4000))\n",
        "    else:\n",
        "        data = load_dataset(\n",
        "            \"json\",\n",
        "            data_files=\"alpaca-tw_en_instruction.json\"\n",
        "        )\n",
        "        train_data = data['train'].shuffle().map(generate_and_tokenize_prompt)\n",
        "        val_data = None\n",
        "\n",
        "    #tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./chatbot_model\",\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=100,\n",
        "        save_steps=100,\n",
        "        save_total_limit=1,\n",
        "        evaluation_strategy=\"no\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[WandbUploadCallback()]\n",
        "    )\n",
        "    # 開始訓練\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "XdbUNNGga1nT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}